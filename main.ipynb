{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5qulr6TsDTG",
    "outputId": "7a5b9f8c-77d8-4d2c-8dbf-f9668cb9b143"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TCDqh3osKad",
    "outputId": "799990b5-fea2-404a-f2e6-cd17270b1f60"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hironcode/shadow-play-segment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBOb1aRN-2eO",
    "outputId": "5677727b-74a9-4034-f73e-43680fef6fa0"
   },
   "outputs": [],
   "source": [
    "!pip install kornia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAZayYCfs0qD"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForImageSegmentation\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "from wasabi import msg\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymDDbDMe6rYs"
   },
   "outputs": [],
   "source": [
    "def init_model(config):\n",
    "    model = AutoModelForImageSegmentation.from_pretrained(config['model']['model'], trust_remote_code=True)\n",
    "    # torch.set_float32_matmul_precision(['high', 'highest'][0])\n",
    "    model.to('cuda')\n",
    "    model.eval()\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "    image_size = tuple(config['model']['image_size'])\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return model, transform_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WYuSwEGs7UD"
   },
   "outputs": [],
   "source": [
    "def get(video_path):\n",
    "    # Use cv2 to read the video at fps=30\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    result = {\n",
    "        \"fps\": fps,\n",
    "        \"width\": cap.get(cv2.CAP_PROP_FRAME_WIDTH),\n",
    "        \"height\": cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    }\n",
    "    cap.release()\n",
    "    del cap\n",
    "    return frames, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qumWAKictJlo"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(frames, fps_old, fps_new, transform_image=None):\n",
    "    # Load the image from the URL\n",
    "    ratio = int(fps_old) // fps_new\n",
    "    frames = frames[::ratio]\n",
    "    if transform_image:\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] = transform_image(frames[i]).unsqueeze(0).to('cuda').to(torch.bfloat16)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3YlvlxqtL_A"
   },
   "outputs": [],
   "source": [
    "def segment(frames, model, is_model=True):\n",
    "    results = []\n",
    "    if is_model:\n",
    "        batch = 1\n",
    "        for i in tqdm(range(0, len(frames), batch)):\n",
    "            preds = model(frames[i])[-1].sigmoid().cpu()\n",
    "            results.append(preds)\n",
    "            del preds\n",
    "        return results\n",
    "    else:\n",
    "        for frame in tqdm(frames):\n",
    "            results.append(pipeline(frame))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I41iZkFuznrL"
   },
   "outputs": [],
   "source": [
    "def save_frames(preds, path, frames=None, is_model=True):\n",
    "\n",
    "    if is_model:\n",
    "        if frames is None:\n",
    "            raise ValueError(\"frames must be provided if is_model is True\")\n",
    "        for i in tqdm(range(len(preds))):\n",
    "            pred = preds[i].squeeze()\n",
    "            pred_pil = transforms.ToPILImage()(pred)\n",
    "            mask = pred_pil.resize(frames[i].size)\n",
    "            frames[i].putalpha(mask)\n",
    "            mask.save(os.path.join(path, f'frame_{i}.png'))\n",
    "\n",
    "    else:\n",
    "        frame_len = len(preds)\n",
    "\n",
    "        for i in range(frame_len):\n",
    "            objects_num = len(preds[i])\n",
    "            for j in range(objects_num):\n",
    "                mask = preds[i][j]['mask']\n",
    "                label = preds[i][j]['label']\n",
    "                mask.save(os.path.join(path, f'{label}{j}_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YozSNpHws5zS"
   },
   "outputs": [],
   "source": [
    "def save_video(config, weight, height):\n",
    "\n",
    "    frame_dir = config['output']['frame_dir']\n",
    "\n",
    "    # Make the frames of the masks into video\n",
    "    video = cv2.VideoWriter(config['output']['output_path'], cv2.VideoWriter_fourcc(*'mp4v'), config['output']['fps'], (weight, height))\n",
    "\n",
    "    # target_label = config['model']['label']\n",
    "\n",
    "    for file in os.listdir(frame_dir).sort(key=lambda x: int(x.split('_')[1].split('.')[0])):\n",
    "        # if target_label not in file:\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     img = Image.open(os.path.join(frame_dir, file)).convert('RGB')\n",
    "        #     video.write(img)\n",
    "        img = Image.open(os.path.join(frame_dir, file)).convert('RGB')\n",
    "        video.write(img)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VG8DCmHhs9uf"
   },
   "outputs": [],
   "source": [
    "def main(config_path):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # Efficient matrix multiplies\n",
    "    subprocess.run(\"export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # load config\n",
    "    config = yaml.safe_load(open(config_path))\n",
    "    output_dir = os.path.dirname(config['output']['output_dir'])\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    video_name = os.path.basename(config['input']['video_path'])\n",
    "    config['output']['output_dir'] = os.path.join(output_dir, video_name)\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    # as of now, only \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\" is supported for this usage\n",
    "    # semantic_segmentation = pipeline(\"image-segmentation\", config['model'])\n",
    "\n",
    "    # init model\n",
    "    model, transform_image = init_model(config)\n",
    "\n",
    "    frames, etc = get(config['input'][\"video_path\"])\n",
    "    frames = preprocess(frames, etc['fps'], config['output']['fps'], transform_image)\n",
    "\n",
    "    msg.info(\"Segmentation Started...\")\n",
    "    with torch.no_grad():\n",
    "        segments = segment(frames, model)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    frame_dir = os.path.join(config['output']['output_dir'], 'frames')\n",
    "    config['output']['frame_dir'] = frame_dir\n",
    "    if not os.path.exists(frame_dir):\n",
    "        os.makedirs(frame_dir)\n",
    "    save_frames(segments, frame_dir)\n",
    "\n",
    "    # delete big tensor\n",
    "    del segments\n",
    "\n",
    "    config['output']['width'] = etc['width']\n",
    "    config['output']['height'] = etc['height']\n",
    "\n",
    "    with open(os.path.join(output_dir, 'config.yaml'), 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    save_video(config, etc['width'], etc['height'])\n",
    "    msg.info(\"Video saved at\", config['output']['output_path'])\n",
    "    del model, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "C_9ddYl2s-5O",
    "outputId": "f938a878-dd16-486b-a858-025b64e5f73d"
   },
   "outputs": [],
   "source": [
    "main(\"shadow-play-segment/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7aX0cIYEPIs"
   },
   "outputs": [],
   "source": [
    "! export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SETBZlZ2tNW3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
